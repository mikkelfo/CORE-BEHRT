env: local
paths:
  # data_path: 'pretrain_datasets/diagnosis_medication/070623/outputs/data' # inside mounted datastore
  data_path: "..\\outputs\\features" # C:\Users\fjn197\PhD\projects\PHAIR\pipelines\ehr2vec-azure-std\ehr2vec\outputs\data_icd10 # inside mounted datastore
  output_path: "../outputs/h_pretraining"
  hierarchical_dir: hierarchical_filtered
  type: med_diag
  run_name: test
  tokenized_dir: tokenized
  # model_path: outputs/h_pretraining/test
  
data:
  dataset:
    select_ratio: 1.
    masking_ratio: 0.8
    replace_ratio: 0.2
    ignore_special_tokens: true
  truncation_len: 20
  num_train_patients: 100
  num_val_patients: 20  
  remove_background: false
  min_len: 2


trainer_args:
  batch_size: 64
  effective_batch_size: 128
  epochs: 5
  info: true
  sampler: null
  gradient_clip: 
    clip_value: 1.0
  mixed_precision: false
  shuffle: true

model:
  # type_vocab_size should be > truncation_len//2 if sep token else >truncation len
  # !!! If you want to feed longer sequences during finetuning adjust type_vocab_size accordingly
  linear: true
  hidden_size: 96
  num_hidden_layers: 3
  num_attention_heads: 3
  intermediate_size: 64
  type_vocab_size: 40
  trainable_level_weights: true
  level_weights: [.8, .6, .4, .2]

optimizer:
  lr: 5e-4
  weight_decay: 0
  eps: 1e-6

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: 10
  num_training_steps: 100
metrics:
  null